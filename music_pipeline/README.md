### Classification

In the project,  worked with the `churn_df` dataset to build a simple classification model using k-Nearest Neighbors (KNN).  specifically focused on the "account_length" and "customer_service_calls" as our features to predict the target variable "churn". Our goal was to convert these features and the target variable into NumPy arrays, which is a typical requirement for machine learning libraries like Scikit-Learn. Once our data was appropriately preprocessed,  created an instance of the KNN classifier and trained it on our data. So, in essence,  leveraged the KNN algorithm to analyze the relationship between account length and customer service calls, and how these factors might have influenced customer churn.

After fitting our k-Nearest Neighbors (KNN) classifier,  utilized it to predict the label of new data points. For training,  employed all the available data. Fortunately, there were new observations available, preloaded for us as X_new.  had the model knn, which  created and fit the data in a previous exercise, preloaded.  used our classifier to predict the labels of a new set of data points, X_new, represented as a numpy array. The X_new array included three new data points with values for "account_length" and "customer_service_calls". Through this exercise,  demonstrated the practical application of our trained KNN model in predicting churn based on new, unseen data.

 were provided with NumPy arrays containing the features as `X` and the target variable as `y`.  divided these into training and test sets, fitted a KNN classifier to the training data, and then computed its accuracy on the test data using the `.score()` method. This exercise gave us a practical experience in applying the train/test split methodology and enabled us to evaluate our KNN model's performance by measuring its accuracy on unseen test data.

Our objective was to build a model that could comprehend the relationship between features and the target variable, while also being able to generalize well when introduced to new observations. To achieve this,  generated accuracy scores for both the training and test sets using a KNN classifier.  varied the `n_neighbors` parameter, the results of which  plotted in a subsequent exercise. For this task,  worked with the training and test sets created from the `churn_df` dataset, preloaded as `X_train`, `X_test`, `y_train`, and `y_test`.  utilized the `KNeighborsClassifier` alongside `numpy` as `np`. Through this process,  gained a hands-on understanding of the concepts of overfitting and underfitting, as well as the importance of choosing the right level of complexity in a machine learning model.

After calculating the accuracy of the KNN model on the training and test sets using various values of `n_neighbors`, created a model complexity curve. This curve allowed us to visualize how the performance of our model changed as adjusted its complexity. To achieve this, utilized the variables `neighbors`, `train_accuracies`, and `test_accuracies`, all of which generated in a prior exercise. plotted these results to assist in finding the optimal number of neighbors for our model. By observing this curve, could see the trade-off between bias and variance as adjusted the `n_neighbors` parameter, which ultimately helped us fine-tune our model for better performance.

![download](https://github.com/ai-akuma/MachineLearning/assets/7444521/8d420723-3a6e-4f5f-8b2e-8ff1c93e4985)

### Regression

Worked with a dataset called `sales_df`, which contained information about advertising campaign expenditure across different media types and the corresponding number of dollars generated in sales for each campaign. The dataset included columns like `tv`, `radio`, `social_media`, and `sales`. The focus was primarily on the "radio" column, using its advertising expenditure as features to predict sales values.  started by creating the feature and target arrays and reshaping them to the format that scikit-learn requires. This process marked our initial steps in feature engineering and preparing data for predictive modeling.

After creating the feature and target arrays, I trained a linear regression model on all feature and target values. Given that the goal was to assess the relationship between the feature and target values, there was no need to split the data into training and test sets. I used the `sales` column as the target variable `y` and the `radio` column as the feature variable `X`, which was reshaped into a 2D array to fit the requirements of scikit-learn. The linear regression model provided a statistical analysis of the relationship between advertising expenditure on radio and the sales generated from it.

Once I built and trained the linear regression model using all available observations, I visualized its performance to interpret the relationship between radio advertising expenditure and sales values. For this visualization, I used `X`, an array of radio values, `y`, an array of sales values, and `predictions`, an array of the model's predicted values for `y` given `X`. The visualization helped in understanding how closely the predictions align with the actual sales values, indicating the effectiveness of radio advertising in influencing sales.
